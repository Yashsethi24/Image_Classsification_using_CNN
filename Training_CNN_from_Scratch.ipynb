{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vwb2RZtVuzbV"
      },
      "outputs": [],
      "source": [
        "# Image Classification with Convolutional Neural Networks\n",
        "# Instructions\n",
        "# Assignment 1: Image Classification with Convolutional Neural Networks\n",
        "\n",
        "# Objective: The goal of this assignment is to deepen your understanding of Convolutional Neural Networks (CNNs) by implementing a robust, advanced architecture for image classification.\n",
        "# You will explore advanced CNN concepts such as dropout, batch normalization, data augmentation, and residual connections while benchmarking your model's performance against existing methods."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset: CIFAR-10 or CIFAR-100. These datasets are widely used in the machine learning community and will allow you to compare your model's performance with reported benchmarks"
      ],
      "metadata": {
        "id": "1nykwPcevWVa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "jbpr9U92PUC6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing"
      ],
      "metadata": {
        "id": "qS7F92mAg3Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. Load and preprocess the dataset, including normalization and data augmentation techniques such as random cropping, horizontal flipping, and color jittering to improve generalization."
      ],
      "metadata": {
        "id": "XXfGXoNdg7qH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the mean and standard deviation of the training dataset to normalize the training and test datasets"
      ],
      "metadata": {
        "id": "SlQQYQLRe5l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-100 dataset without any transformations\n",
        "dataset = datasets.CIFAR100(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=True,  # Use the training set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transforms.ToTensor(),  # Convert images to tensors\n",
        ")\n",
        "\n",
        "# Create a DataLoader to iterate through the dataset\n",
        "data_loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
        "\n",
        "# Initialize variables to accumulate pixel values\n",
        "mean = torch.zeros(3)  # Mean for 3 channels (R, G, B)\n",
        "std = torch.zeros(3)   # Standard deviation for 3 channels (R, G, B)\n",
        "\n",
        "# Iterate through the dataset to compute mean and std\n",
        "for images, _ in data_loader:\n",
        "    # Compute mean and std for each channel\n",
        "    for i in range(3):  # Loop over the 3 channels (R, G, B)\n",
        "        mean[i] = images[:, i, :, :].mean()\n",
        "        std[i] = images[:, i, :, :].std()\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Standard Deviation: {std}\")"
      ],
      "metadata": {
        "id": "ncTAu1VBe4xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the train and test dataset with following transformations:\n",
        "\n",
        "#### Train dataset:\n",
        "1.   Horizontal Flip\n",
        "2.   Random Cropping\n",
        "3.   Color Jittering\n",
        "4.   Converting to tensor\n",
        "5.   Normalizing using the mean and standard deviation obtained above\n",
        "\n",
        "#### Test dataset:\n",
        "1.   Converting to tensor\n",
        "2.   Normalizing using the mean and standard deviation obtained above"
      ],
      "metadata": {
        "id": "Myf0WNUwfPOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the training dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.RandomCrop(32, padding=4),  # Randomly crop the image\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color Jittering\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean, std),  # Normalize with CIFAR-100 mean and std\n",
        "])\n",
        "\n",
        "# Define transformations for the testing dataset\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean, std),  # Normalize with CIFAR-100 mean and std\n",
        "])\n",
        "\n",
        "# Load the CIFAR-100 training dataset\n",
        "train_dataset = datasets.CIFAR100(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=True,  # Load the training set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transform_train,  # Apply the training transformations\n",
        ")\n",
        "\n",
        "# Load the CIFAR-100 testing dataset\n",
        "test_dataset = datasets.CIFAR100(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=False,  # Load the testing set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transform_test,  # Apply the testing transformations\n",
        ")\n",
        "\n",
        "# Create data loaders for training and testing\n",
        "batch_size = 64  # Number of samples per batch\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle the training data\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # Do not shuffle the testing data\n",
        "\n",
        "# Print dataset details\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8ue-c85cAn3",
        "outputId": "086a4b02-167e-4928-bdde-ad38d85f9662"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 50000\n",
            "Testing dataset size: 10000\n",
            "Number of classes: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Convert labels into one-hot encoded vectors and split the data into training, validation, and testing subsets."
      ],
      "metadata": {
        "id": "S4bz5leug_wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to one-hot encoded vectors\n",
        "def one_hot_encode(labels, num_classes=100):\n",
        "    return F.one_hot(torch.tensor(labels), num_classes=num_classes)\n",
        "\n",
        "# Apply one-hot encoding to the training and testing labels\n",
        "train_dataset.targets = one_hot_encode(train_dataset.targets)\n",
        "test_dataset.targets = one_hot_encode(test_dataset.targets)\n",
        "\n",
        "# Split the training dataset into training and validation subsets\n",
        "train_size = int(0.8 * len(train_dataset))  # 80% for training\n",
        "val_size = len(train_dataset) - train_size  # 20% for validation\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "batch_size = 64  # Number of samples per batch\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle the training data\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # Do not shuffle the validation data\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # Do not shuffle the testing data\n",
        "\n",
        "# Print dataset details\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
        "print(f\"Number of classes: {len(train_dataset.dataset.targets[0])}\")  # Number of classes (100 for CIFAR-100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUlroT8zgI8x",
        "outputId": "55acd7f0-5bae-4b9d-c68b-3c6f974dbdba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 40000\n",
            "Validation dataset size: 10000\n",
            "Testing dataset size: 10000\n",
            "Number of classes: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, activation='relu'):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut connection if input and output dimensions don't match\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        # Activation function\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU(inplace=True)\n",
        "        elif activation == 'leaky_relu':\n",
        "            self.activation = nn.LeakyReLU(0.1, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.activation(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(x)  # Residual connection\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(CNN, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1, activation='relu')\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2, activation='leaky_relu')\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2, activation='relu')\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2, activation='leaky_relu')\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride, activation):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(ResidualBlock(self.in_channels, out_channels, stride, activation))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))  # Global average pooling\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = CNN(num_classes=100)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsHF-X9ngIza",
        "outputId": "2eb0c416-847e-4ff1-fdf6-f87e954bade1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "      (activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "      (activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "      (activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        # Convert one-hot encoded labels to class indices\n",
        "        labels = torch.argmax(labels, dim=1)  # Shape: (batch_size,)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)  # Use class indices for loss calculation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwmOs1MbPpLK",
        "outputId": "6e99cc1c-6280-4465-8850-b2a54f407cce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.180600309753418\n",
            "Epoch [2/10], Loss: 3.7010858730316163\n",
            "Epoch [3/10], Loss: 3.4149109748840334\n",
            "Epoch [4/10], Loss: 3.206293747329712\n",
            "Epoch [5/10], Loss: 3.031036769104004\n",
            "Epoch [6/10], Loss: 2.8902068378448487\n",
            "Epoch [7/10], Loss: 2.781661339187622\n",
            "Epoch [8/10], Loss: 2.6750270671844483\n",
            "Epoch [9/10], Loss: 2.5879315204620363\n",
            "Epoch [10/10], Loss: 2.515890392112732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define the model, optimizer, and loss function\n",
        "model = CNN(num_classes=100).to(device)  # Move model to GPU\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Lists to store training/validation loss and accuracy\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Move data to GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Convert one-hot encoded labels to class indices\n",
        "        labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate training loss and accuracy\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct / total\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            # Move data to GPU\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Convert one-hot encoded labels to class indices\n",
        "            labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate validation loss and accuracy\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print training and validation results\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Training Loss: {train_loss:.4f}, \"\n",
        "          f\"Validation Loss: {val_loss:.4f}, \"\n",
        "          f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# Plot loss curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Validation Accuracy', color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "6qnj0Pzml-bJ",
        "outputId": "4dd673d5-1550-4c41-cf19-e57189c827a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Training Loss: 4.1893, Validation Loss: 3.8777, Validation Accuracy: 9.85%\n",
            "Epoch [2/20], Training Loss: 3.6933, Validation Loss: 3.4039, Validation Accuracy: 17.58%\n",
            "Epoch [3/20], Training Loss: 3.3985, Validation Loss: 3.2181, Validation Accuracy: 20.50%\n",
            "Epoch [4/20], Training Loss: 3.1708, Validation Loss: 3.0526, Validation Accuracy: 24.26%\n",
            "Epoch [5/20], Training Loss: 3.0010, Validation Loss: 2.8374, Validation Accuracy: 28.10%\n",
            "Epoch [6/20], Training Loss: 2.8649, Validation Loss: 2.6898, Validation Accuracy: 31.17%\n",
            "Epoch [7/20], Training Loss: 2.7479, Validation Loss: 2.6835, Validation Accuracy: 30.76%\n",
            "Epoch [8/20], Training Loss: 2.6542, Validation Loss: 2.5723, Validation Accuracy: 33.21%\n",
            "Epoch [9/20], Training Loss: 2.5648, Validation Loss: 2.4985, Validation Accuracy: 35.59%\n",
            "Epoch [10/20], Training Loss: 2.4934, Validation Loss: 2.4539, Validation Accuracy: 35.93%\n",
            "Epoch [11/20], Training Loss: 2.4195, Validation Loss: 2.3593, Validation Accuracy: 37.99%\n",
            "Epoch [12/20], Training Loss: 2.3562, Validation Loss: 2.3152, Validation Accuracy: 39.07%\n",
            "Epoch [13/20], Training Loss: 2.2919, Validation Loss: 2.2974, Validation Accuracy: 39.96%\n",
            "Epoch [14/20], Training Loss: 2.2388, Validation Loss: 2.2840, Validation Accuracy: 39.63%\n",
            "Epoch [15/20], Training Loss: 2.1932, Validation Loss: 2.2255, Validation Accuracy: 41.45%\n",
            "Epoch [16/20], Training Loss: 2.1432, Validation Loss: 2.1826, Validation Accuracy: 42.36%\n",
            "Epoch [17/20], Training Loss: 2.0962, Validation Loss: 2.2013, Validation Accuracy: 41.63%\n",
            "Epoch [18/20], Training Loss: 2.0579, Validation Loss: 2.1192, Validation Accuracy: 44.14%\n",
            "Epoch [19/20], Training Loss: 2.0198, Validation Loss: 2.1017, Validation Accuracy: 44.10%\n",
            "Epoch [20/20], Training Loss: 1.9727, Validation Loss: 2.1035, Validation Accuracy: 44.62%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ubFTUw3ItnUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-10 Dataset"
      ],
      "metadata": {
        "id": "tetrtRq3RpoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define transformations for the training and testing datasets\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.RandomCrop(32, padding=4),  # Randomly crop the image\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),  # Normalize with CIFAR-10 mean and std\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),  # Normalize with CIFAR-10 mean and std\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 training dataset\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=True,  # Load the training set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transform_train,  # Apply the training transformations\n",
        ")\n",
        "\n",
        "# Load the CIFAR-10 testing dataset\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=False,  # Load the testing set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transform_test,  # Apply the testing transformations\n",
        ")\n",
        "\n",
        "# Split the training dataset into training and validation subsets\n",
        "train_size = int(0.8 * len(train_dataset))  # 80% for training\n",
        "val_size = len(train_dataset) - train_size  # 20% for validation\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, activation='relu', kernel_size=3, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.activation = activation\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.fc1 = nn.Linear(256 * 8 * 8, 1024)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.apply_activation(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(self.apply_activation(self.bn2(self.conv2(x))))\n",
        "        x = self.apply_activation(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(self.apply_activation(self.bn4(self.conv4(x))))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(self.apply_activation(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def apply_activation(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return F.relu(x)\n",
        "        elif self.activation == 'leaky_relu':\n",
        "            return F.leaky_relu(x, negative_slope=0.1)\n",
        "        elif self.activation == 'gelu':\n",
        "            return F.gelu(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {self.activation}\")\n",
        "\n",
        "# Hyperparameters to tune\n",
        "activations = ['relu', 'leaky_relu', 'gelu']\n",
        "kernel_sizes = [3, 5]\n",
        "optimizers_list = ['adam', 'sgd']\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Hyperparameter tuning loop\n",
        "for activation in activations:\n",
        "    for kernel_size in kernel_sizes:\n",
        "        for optimizer_name in optimizers_list:\n",
        "            print(f\"\\nTraining with activation={activation}, kernel_size={kernel_size}, optimizer={optimizer_name}\")\n",
        "\n",
        "            # Create the model\n",
        "            model = CNN(activation=activation, kernel_size=kernel_size, num_classes=10).to(device)\n",
        "\n",
        "            # Define loss function and optimizer\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            if optimizer_name == 'adam':\n",
        "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "            elif optimizer_name == 'sgd':\n",
        "                optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "            # Training loop\n",
        "            num_epochs = 10  # Train for fewer epochs for tuning\n",
        "            best_val_accuracy = 0.0\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "                train_loss = running_loss / len(train_loader)\n",
        "                train_accuracy = 100 * correct / total\n",
        "\n",
        "                # Validation loop\n",
        "                model.eval()\n",
        "                val_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for inputs, labels in val_loader:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item()\n",
        "\n",
        "                        _, predicted = torch.max(outputs, 1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += (predicted == labels).sum().item()\n",
        "\n",
        "                val_loss = val_loss / len(val_loader)\n",
        "                val_accuracy = 100 * correct / total\n",
        "\n",
        "                # Save the best validation accuracy\n",
        "                if val_accuracy > best_val_accuracy:\n",
        "                    best_val_accuracy = val_accuracy\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "                      f\"Training Loss: {train_loss:.4f}, \"\n",
        "                      f\"Validation Loss: {val_loss:.4f}, \"\n",
        "                      f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "                # Step the learning rate scheduler\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "            # Store the results\n",
        "            results.append({\n",
        "                'activation': activation,\n",
        "                'kernel_size': kernel_size,\n",
        "                'optimizer': optimizer_name,\n",
        "                'best_val_accuracy': best_val_accuracy\n",
        "            })\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nHyperparameter Tuning Results:\")\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "# Find the best model\n",
        "best_result = max(results, key=lambda x: x['best_val_accuracy'])\n",
        "print(f\"\\nBest Model: {best_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPolJTiU8rag",
        "outputId": "df1afdd2-a4fe-4147-8572-9f8a27172bb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 170M/170M [00:13<00:00, 12.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with activation=relu, kernel_size=3, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 2.3096, Validation Loss: 1.7219, Validation Accuracy: 36.41%\n",
            "Epoch [2/10], Training Loss: 1.6126, Validation Loss: 1.9599, Validation Accuracy: 32.98%\n",
            "Epoch [3/10], Training Loss: 1.4488, Validation Loss: 1.3738, Validation Accuracy: 51.89%\n",
            "Epoch [4/10], Training Loss: 1.3193, Validation Loss: 1.2397, Validation Accuracy: 53.84%\n",
            "Epoch [5/10], Training Loss: 1.2183, Validation Loss: 1.0688, Validation Accuracy: 61.81%\n",
            "Epoch [6/10], Training Loss: 1.1360, Validation Loss: 1.0769, Validation Accuracy: 61.57%\n",
            "Epoch [7/10], Training Loss: 1.0751, Validation Loss: 1.0222, Validation Accuracy: 64.32%\n",
            "Epoch [8/10], Training Loss: 1.0289, Validation Loss: 1.0889, Validation Accuracy: 63.85%\n",
            "Epoch [9/10], Training Loss: 0.9810, Validation Loss: 1.0203, Validation Accuracy: 64.18%\n",
            "Epoch [10/10], Training Loss: 0.9472, Validation Loss: 0.8451, Validation Accuracy: 70.31%\n",
            "\n",
            "Training with activation=relu, kernel_size=3, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 1.7178, Validation Loss: 1.4215, Validation Accuracy: 47.01%\n",
            "Epoch [2/10], Training Loss: 1.4901, Validation Loss: 1.4133, Validation Accuracy: 49.36%\n",
            "Epoch [3/10], Training Loss: 1.3401, Validation Loss: 1.5841, Validation Accuracy: 48.87%\n",
            "Epoch [4/10], Training Loss: 1.2523, Validation Loss: 1.0795, Validation Accuracy: 61.55%\n",
            "Epoch [5/10], Training Loss: 1.1650, Validation Loss: 1.0425, Validation Accuracy: 63.42%\n",
            "Epoch [6/10], Training Loss: 1.1044, Validation Loss: 1.0192, Validation Accuracy: 64.98%\n",
            "Epoch [7/10], Training Loss: 1.0338, Validation Loss: 1.1244, Validation Accuracy: 62.62%\n",
            "Epoch [8/10], Training Loss: 0.9940, Validation Loss: 0.9443, Validation Accuracy: 67.86%\n",
            "Epoch [9/10], Training Loss: 0.9468, Validation Loss: 0.9513, Validation Accuracy: 67.24%\n",
            "Epoch [10/10], Training Loss: 0.9024, Validation Loss: 0.9602, Validation Accuracy: 68.30%\n",
            "\n",
            "Training with activation=relu, kernel_size=5, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 2.2406, Validation Loss: 1.6880, Validation Accuracy: 37.58%\n",
            "Epoch [2/10], Training Loss: 1.6486, Validation Loss: 1.4744, Validation Accuracy: 45.63%\n",
            "Epoch [3/10], Training Loss: 1.4965, Validation Loss: 1.3142, Validation Accuracy: 52.67%\n",
            "Epoch [4/10], Training Loss: 1.3310, Validation Loss: 1.1510, Validation Accuracy: 58.81%\n",
            "Epoch [5/10], Training Loss: 1.2174, Validation Loss: 1.0259, Validation Accuracy: 62.65%\n",
            "Epoch [6/10], Training Loss: 1.1267, Validation Loss: 0.9514, Validation Accuracy: 65.96%\n",
            "Epoch [7/10], Training Loss: 1.0573, Validation Loss: 0.9885, Validation Accuracy: 64.14%\n",
            "Epoch [8/10], Training Loss: 0.9827, Validation Loss: 0.8748, Validation Accuracy: 68.90%\n",
            "Epoch [9/10], Training Loss: 0.9280, Validation Loss: 0.8054, Validation Accuracy: 71.68%\n",
            "Epoch [10/10], Training Loss: 0.8792, Validation Loss: 0.7972, Validation Accuracy: 71.96%\n",
            "\n",
            "Training with activation=relu, kernel_size=5, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 1.7399, Validation Loss: 1.5025, Validation Accuracy: 44.82%\n",
            "Epoch [2/10], Training Loss: 1.5059, Validation Loss: 1.3154, Validation Accuracy: 52.11%\n",
            "Epoch [3/10], Training Loss: 1.3311, Validation Loss: 1.2901, Validation Accuracy: 53.02%\n",
            "Epoch [4/10], Training Loss: 1.2137, Validation Loss: 1.0752, Validation Accuracy: 60.87%\n",
            "Epoch [5/10], Training Loss: 1.1115, Validation Loss: 0.9513, Validation Accuracy: 65.69%\n",
            "Epoch [6/10], Training Loss: 1.0253, Validation Loss: 0.9277, Validation Accuracy: 67.72%\n",
            "Epoch [7/10], Training Loss: 0.9541, Validation Loss: 1.0281, Validation Accuracy: 64.42%\n",
            "Epoch [8/10], Training Loss: 0.9089, Validation Loss: 0.8228, Validation Accuracy: 70.99%\n",
            "Epoch [9/10], Training Loss: 0.8454, Validation Loss: 0.8129, Validation Accuracy: 71.45%\n",
            "Epoch [10/10], Training Loss: 0.7946, Validation Loss: 0.8577, Validation Accuracy: 70.80%\n",
            "\n",
            "Training with activation=leaky_relu, kernel_size=3, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 2.6083, Validation Loss: 1.6193, Validation Accuracy: 40.05%\n",
            "Epoch [2/10], Training Loss: 1.5317, Validation Loss: 1.4196, Validation Accuracy: 47.80%\n",
            "Epoch [3/10], Training Loss: 1.3065, Validation Loss: 1.1873, Validation Accuracy: 58.23%\n",
            "Epoch [4/10], Training Loss: 1.1431, Validation Loss: 1.0439, Validation Accuracy: 62.77%\n",
            "Epoch [5/10], Training Loss: 1.0199, Validation Loss: 0.9593, Validation Accuracy: 66.43%\n",
            "Epoch [6/10], Training Loss: 0.9336, Validation Loss: 0.8759, Validation Accuracy: 69.00%\n",
            "Epoch [7/10], Training Loss: 0.8667, Validation Loss: 0.9041, Validation Accuracy: 68.42%\n",
            "Epoch [8/10], Training Loss: 0.8116, Validation Loss: 0.8827, Validation Accuracy: 69.89%\n",
            "Epoch [9/10], Training Loss: 0.7687, Validation Loss: 0.7151, Validation Accuracy: 74.68%\n",
            "Epoch [10/10], Training Loss: 0.7298, Validation Loss: 0.7308, Validation Accuracy: 74.90%\n",
            "\n",
            "Training with activation=leaky_relu, kernel_size=3, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 1.6210, Validation Loss: 1.5768, Validation Accuracy: 44.09%\n",
            "Epoch [2/10], Training Loss: 1.2977, Validation Loss: 1.1373, Validation Accuracy: 59.61%\n",
            "Epoch [3/10], Training Loss: 1.1083, Validation Loss: 0.9869, Validation Accuracy: 65.67%\n",
            "Epoch [4/10], Training Loss: 0.9912, Validation Loss: 1.1247, Validation Accuracy: 61.63%\n",
            "Epoch [5/10], Training Loss: 0.9224, Validation Loss: 0.8742, Validation Accuracy: 68.82%\n",
            "Epoch [6/10], Training Loss: 0.8629, Validation Loss: 0.8590, Validation Accuracy: 68.97%\n",
            "Epoch [7/10], Training Loss: 0.8131, Validation Loss: 0.7922, Validation Accuracy: 72.18%\n",
            "Epoch [8/10], Training Loss: 0.7661, Validation Loss: 0.7315, Validation Accuracy: 74.37%\n",
            "Epoch [9/10], Training Loss: 0.7309, Validation Loss: 0.7954, Validation Accuracy: 72.54%\n",
            "Epoch [10/10], Training Loss: 0.6959, Validation Loss: 0.7505, Validation Accuracy: 73.87%\n",
            "\n",
            "Training with activation=leaky_relu, kernel_size=5, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 2.4578, Validation Loss: 1.6959, Validation Accuracy: 37.05%\n",
            "Epoch [2/10], Training Loss: 1.5807, Validation Loss: 1.5354, Validation Accuracy: 45.28%\n",
            "Epoch [3/10], Training Loss: 1.3202, Validation Loss: 1.2184, Validation Accuracy: 55.79%\n",
            "Epoch [4/10], Training Loss: 1.1501, Validation Loss: 1.1391, Validation Accuracy: 59.61%\n",
            "Epoch [5/10], Training Loss: 1.0162, Validation Loss: 0.9385, Validation Accuracy: 66.81%\n",
            "Epoch [6/10], Training Loss: 0.9257, Validation Loss: 0.8605, Validation Accuracy: 69.39%\n",
            "Epoch [7/10], Training Loss: 0.8417, Validation Loss: 0.8682, Validation Accuracy: 69.95%\n",
            "Epoch [8/10], Training Loss: 0.7849, Validation Loss: 0.7387, Validation Accuracy: 74.47%\n",
            "Epoch [9/10], Training Loss: 0.7304, Validation Loss: 0.7368, Validation Accuracy: 74.30%\n",
            "Epoch [10/10], Training Loss: 0.6919, Validation Loss: 0.7042, Validation Accuracy: 75.80%\n",
            "\n",
            "Training with activation=leaky_relu, kernel_size=5, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 1.6460, Validation Loss: 1.4264, Validation Accuracy: 46.55%\n",
            "Epoch [2/10], Training Loss: 1.2856, Validation Loss: 1.2006, Validation Accuracy: 57.69%\n",
            "Epoch [3/10], Training Loss: 1.0859, Validation Loss: 1.0249, Validation Accuracy: 63.49%\n",
            "Epoch [4/10], Training Loss: 0.9786, Validation Loss: 0.9443, Validation Accuracy: 66.42%\n",
            "Epoch [5/10], Training Loss: 0.8923, Validation Loss: 0.9123, Validation Accuracy: 67.26%\n",
            "Epoch [6/10], Training Loss: 0.8294, Validation Loss: 0.8540, Validation Accuracy: 69.63%\n",
            "Epoch [7/10], Training Loss: 0.7795, Validation Loss: 0.7551, Validation Accuracy: 73.24%\n",
            "Epoch [8/10], Training Loss: 0.7313, Validation Loss: 0.9593, Validation Accuracy: 67.58%\n",
            "Epoch [9/10], Training Loss: 0.6972, Validation Loss: 0.7305, Validation Accuracy: 74.70%\n",
            "Epoch [10/10], Training Loss: 0.6596, Validation Loss: 0.6488, Validation Accuracy: 77.45%\n",
            "\n",
            "Training with activation=gelu, kernel_size=3, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 1.9755, Validation Loss: 1.4908, Validation Accuracy: 43.89%\n",
            "Epoch [2/10], Training Loss: 1.4940, Validation Loss: 1.3461, Validation Accuracy: 49.71%\n",
            "Epoch [3/10], Training Loss: 1.3146, Validation Loss: 1.2070, Validation Accuracy: 57.54%\n",
            "Epoch [4/10], Training Loss: 1.1981, Validation Loss: 1.0768, Validation Accuracy: 61.54%\n",
            "Epoch [5/10], Training Loss: 1.0969, Validation Loss: 0.9620, Validation Accuracy: 65.73%\n",
            "Epoch [6/10], Training Loss: 1.0090, Validation Loss: 0.8854, Validation Accuracy: 68.97%\n",
            "Epoch [7/10], Training Loss: 0.9512, Validation Loss: 0.8229, Validation Accuracy: 70.82%\n",
            "Epoch [8/10], Training Loss: 0.8879, Validation Loss: 0.8001, Validation Accuracy: 72.14%\n",
            "Epoch [9/10], Training Loss: 0.8471, Validation Loss: 0.8467, Validation Accuracy: 70.89%\n",
            "Epoch [10/10], Training Loss: 0.8013, Validation Loss: 0.7167, Validation Accuracy: 74.98%\n",
            "\n",
            "Training with activation=gelu, kernel_size=3, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 1.6079, Validation Loss: 1.3018, Validation Accuracy: 52.61%\n",
            "Epoch [2/10], Training Loss: 1.3066, Validation Loss: 1.1618, Validation Accuracy: 57.55%\n",
            "Epoch [3/10], Training Loss: 1.1497, Validation Loss: 0.9664, Validation Accuracy: 64.82%\n",
            "Epoch [4/10], Training Loss: 1.0401, Validation Loss: 0.9601, Validation Accuracy: 66.33%\n",
            "Epoch [5/10], Training Loss: 0.9586, Validation Loss: 0.8786, Validation Accuracy: 69.61%\n",
            "Epoch [6/10], Training Loss: 0.8949, Validation Loss: 0.7883, Validation Accuracy: 72.30%\n",
            "Epoch [7/10], Training Loss: 0.8319, Validation Loss: 0.7378, Validation Accuracy: 74.08%\n",
            "Epoch [8/10], Training Loss: 0.7962, Validation Loss: 0.8065, Validation Accuracy: 71.78%\n",
            "Epoch [9/10], Training Loss: 0.7478, Validation Loss: 0.7337, Validation Accuracy: 74.53%\n",
            "Epoch [10/10], Training Loss: 0.7162, Validation Loss: 0.6468, Validation Accuracy: 77.24%\n",
            "\n",
            "Training with activation=gelu, kernel_size=5, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 2.1079, Validation Loss: 1.6214, Validation Accuracy: 40.20%\n",
            "Epoch [2/10], Training Loss: 1.5543, Validation Loss: 1.3540, Validation Accuracy: 50.38%\n",
            "Epoch [3/10], Training Loss: 1.3493, Validation Loss: 1.2100, Validation Accuracy: 57.49%\n",
            "Epoch [4/10], Training Loss: 1.1986, Validation Loss: 1.0754, Validation Accuracy: 61.80%\n",
            "Epoch [5/10], Training Loss: 1.0690, Validation Loss: 0.9244, Validation Accuracy: 66.43%\n",
            "Epoch [6/10], Training Loss: 0.9804, Validation Loss: 0.9415, Validation Accuracy: 66.62%\n",
            "Epoch [7/10], Training Loss: 0.9071, Validation Loss: 0.8501, Validation Accuracy: 70.22%\n",
            "Epoch [8/10], Training Loss: 0.8481, Validation Loss: 0.7487, Validation Accuracy: 73.35%\n",
            "Epoch [9/10], Training Loss: 0.7905, Validation Loss: 0.7814, Validation Accuracy: 72.57%\n",
            "Epoch [10/10], Training Loss: 0.7474, Validation Loss: 0.7811, Validation Accuracy: 73.30%\n",
            "\n",
            "Training with activation=gelu, kernel_size=5, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 1.6351, Validation Loss: 1.3785, Validation Accuracy: 49.90%\n",
            "Epoch [2/10], Training Loss: 1.2897, Validation Loss: 1.2390, Validation Accuracy: 55.94%\n",
            "Epoch [3/10], Training Loss: 1.1226, Validation Loss: 1.0742, Validation Accuracy: 61.63%\n",
            "Epoch [4/10], Training Loss: 1.0013, Validation Loss: 1.0323, Validation Accuracy: 65.73%\n",
            "Epoch [5/10], Training Loss: 0.9072, Validation Loss: 0.8222, Validation Accuracy: 70.91%\n",
            "Epoch [6/10], Training Loss: 0.8490, Validation Loss: 0.8182, Validation Accuracy: 71.00%\n",
            "Epoch [7/10], Training Loss: 0.7835, Validation Loss: 0.7169, Validation Accuracy: 75.13%\n",
            "Epoch [8/10], Training Loss: 0.7297, Validation Loss: 0.6875, Validation Accuracy: 75.51%\n",
            "Epoch [9/10], Training Loss: 0.6791, Validation Loss: 0.7211, Validation Accuracy: 75.07%\n",
            "Epoch [10/10], Training Loss: 0.6472, Validation Loss: 0.6790, Validation Accuracy: 76.55%\n",
            "\n",
            "Hyperparameter Tuning Results:\n",
            "{'activation': 'relu', 'kernel_size': 3, 'optimizer': 'adam', 'best_val_accuracy': 70.31}\n",
            "{'activation': 'relu', 'kernel_size': 3, 'optimizer': 'sgd', 'best_val_accuracy': 68.3}\n",
            "{'activation': 'relu', 'kernel_size': 5, 'optimizer': 'adam', 'best_val_accuracy': 71.96}\n",
            "{'activation': 'relu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 71.45}\n",
            "{'activation': 'leaky_relu', 'kernel_size': 3, 'optimizer': 'adam', 'best_val_accuracy': 74.9}\n",
            "{'activation': 'leaky_relu', 'kernel_size': 3, 'optimizer': 'sgd', 'best_val_accuracy': 74.37}\n",
            "{'activation': 'leaky_relu', 'kernel_size': 5, 'optimizer': 'adam', 'best_val_accuracy': 75.8}\n",
            "{'activation': 'leaky_relu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 77.45}\n",
            "{'activation': 'gelu', 'kernel_size': 3, 'optimizer': 'adam', 'best_val_accuracy': 74.98}\n",
            "{'activation': 'gelu', 'kernel_size': 3, 'optimizer': 'sgd', 'best_val_accuracy': 77.24}\n",
            "{'activation': 'gelu', 'kernel_size': 5, 'optimizer': 'adam', 'best_val_accuracy': 73.35}\n",
            "{'activation': 'gelu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 76.55}\n",
            "\n",
            "Best Model: {'activation': 'leaky_relu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 77.45}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define transformations for the training and testing datasets\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.RandomCrop(32, padding=4),  # Randomly crop the image\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),  # Normalize with CIFAR-10 mean and std\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),  # Normalize with CIFAR-10 mean and std\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 training dataset\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=True,  # Load the training set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transform_train,  # Apply the training transformations\n",
        ")\n",
        "\n",
        "# Load the CIFAR-10 testing dataset\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=False,  # Load the testing set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transform_test,  # Apply the testing transformations\n",
        ")\n",
        "\n",
        "# Split the training dataset into training and validation subsets\n",
        "train_size = int(0.8 * len(train_dataset))  # 80% for training\n",
        "val_size = len(train_dataset) - train_size  # 20% for validation\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, activation='relu', kernel_size=3, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.activation = activation\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.fc1 = nn.Linear(256 * 8 * 8, 1024)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.apply_activation(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(self.apply_activation(self.bn2(self.conv2(x))))\n",
        "        x = self.apply_activation(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(self.apply_activation(self.bn4(self.conv4(x))))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(self.apply_activation(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def apply_activation(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return F.relu(x)\n",
        "        elif self.activation == 'leaky_relu':\n",
        "            return F.leaky_relu(x, negative_slope=0.1)\n",
        "        elif self.activation == 'gelu':\n",
        "            return F.gelu(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {self.activation}\")\n",
        "\n",
        "# Hyperparameters to tune\n",
        "activations = [ 'leaky_relu']\n",
        "kernel_sizes = [ 5]\n",
        "optimizers_list = ['sgd']\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Hyperparameter tuning loop\n",
        "for activation in activations:\n",
        "    for kernel_size in kernel_sizes:\n",
        "        for optimizer_name in optimizers_list:\n",
        "            print(f\"\\nTraining with activation={activation}, kernel_size={kernel_size}, optimizer={optimizer_name}\")\n",
        "\n",
        "            # Create the model\n",
        "            model = CNN(activation=activation, kernel_size=kernel_size, num_classes=10).to(device)\n",
        "\n",
        "            # Define loss function and optimizer\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            if optimizer_name == 'adam':\n",
        "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "            elif optimizer_name == 'sgd':\n",
        "                optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "            # Training loop\n",
        "            num_epochs = 30  # Train for fewer epochs for tuning\n",
        "            best_val_accuracy = 0.0\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "                train_loss = running_loss / len(train_loader)\n",
        "                train_accuracy = 100 * correct / total\n",
        "\n",
        "                # Validation loop\n",
        "                model.eval()\n",
        "                val_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for inputs, labels in val_loader:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item()\n",
        "\n",
        "                        _, predicted = torch.max(outputs, 1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += (predicted == labels).sum().item()\n",
        "\n",
        "                val_loss = val_loss / len(val_loader)\n",
        "                val_accuracy = 100 * correct / total\n",
        "\n",
        "                # Save the best validation accuracy\n",
        "                if val_accuracy > best_val_accuracy:\n",
        "                    best_val_accuracy = val_accuracy\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "                      f\"Training Loss: {train_loss:.4f}, \"\n",
        "                      f\"Validation Loss: {val_loss:.4f}, \"\n",
        "                      f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "                # Step the learning rate scheduler\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "            # Store the results\n",
        "            results.append({\n",
        "                'activation': activation,\n",
        "                'kernel_size': kernel_size,\n",
        "                'optimizer': optimizer_name,\n",
        "                'best_val_accuracy': best_val_accuracy\n",
        "            })\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nHyperparameter Tuning Results:\")\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "# Find the best model\n",
        "best_result = max(results, key=lambda x: x['best_val_accuracy'])\n",
        "print(f\"\\nBest Model: {best_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d-bAZBU8qxA",
        "outputId": "fdb8027a-a773-4e2a-f216-863a5eebc965"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Training with activation=leaky_relu, kernel_size=5, optimizer=sgd\n",
            "Epoch [1/30], Training Loss: 1.6493, Validation Loss: 1.4829, Validation Accuracy: 46.40%\n",
            "Epoch [2/30], Training Loss: 1.2849, Validation Loss: 1.5066, Validation Accuracy: 49.41%\n",
            "Epoch [3/30], Training Loss: 1.0853, Validation Loss: 1.0258, Validation Accuracy: 63.78%\n",
            "Epoch [4/30], Training Loss: 0.9784, Validation Loss: 0.9866, Validation Accuracy: 65.33%\n",
            "Epoch [5/30], Training Loss: 0.8960, Validation Loss: 0.9017, Validation Accuracy: 68.54%\n",
            "Epoch [6/30], Training Loss: 0.8318, Validation Loss: 0.8843, Validation Accuracy: 69.82%\n",
            "Epoch [7/30], Training Loss: 0.7733, Validation Loss: 0.7855, Validation Accuracy: 72.81%\n",
            "Epoch [8/30], Training Loss: 0.7317, Validation Loss: 0.8122, Validation Accuracy: 72.23%\n",
            "Epoch [9/30], Training Loss: 0.6982, Validation Loss: 0.7114, Validation Accuracy: 75.25%\n",
            "Epoch [10/30], Training Loss: 0.6603, Validation Loss: 0.6655, Validation Accuracy: 76.58%\n",
            "Epoch [11/30], Training Loss: 0.6359, Validation Loss: 0.6748, Validation Accuracy: 77.73%\n",
            "Epoch [12/30], Training Loss: 0.5956, Validation Loss: 0.6149, Validation Accuracy: 78.72%\n",
            "Epoch [13/30], Training Loss: 0.5723, Validation Loss: 0.5808, Validation Accuracy: 80.41%\n",
            "Epoch [14/30], Training Loss: 0.5503, Validation Loss: 0.7021, Validation Accuracy: 76.99%\n",
            "Epoch [15/30], Training Loss: 0.5181, Validation Loss: 0.6099, Validation Accuracy: 78.92%\n",
            "Epoch [16/30], Training Loss: 0.5032, Validation Loss: 0.6701, Validation Accuracy: 78.02%\n",
            "Epoch [17/30], Training Loss: 0.4776, Validation Loss: 0.5538, Validation Accuracy: 81.11%\n",
            "Epoch [18/30], Training Loss: 0.4611, Validation Loss: 0.5431, Validation Accuracy: 81.83%\n",
            "Epoch [19/30], Training Loss: 0.4430, Validation Loss: 0.4939, Validation Accuracy: 82.96%\n",
            "Epoch [20/30], Training Loss: 0.4233, Validation Loss: 0.4958, Validation Accuracy: 83.35%\n",
            "Epoch [21/30], Training Loss: 0.4114, Validation Loss: 0.5083, Validation Accuracy: 83.12%\n",
            "Epoch [22/30], Training Loss: 0.3962, Validation Loss: 0.5212, Validation Accuracy: 82.70%\n",
            "Epoch [23/30], Training Loss: 0.3745, Validation Loss: 0.4993, Validation Accuracy: 84.04%\n",
            "Epoch [24/30], Training Loss: 0.3036, Validation Loss: 0.3966, Validation Accuracy: 86.62%\n",
            "Epoch [25/30], Training Loss: 0.2884, Validation Loss: 0.3930, Validation Accuracy: 86.75%\n",
            "Epoch [26/30], Training Loss: 0.2774, Validation Loss: 0.3987, Validation Accuracy: 86.40%\n",
            "Epoch [27/30], Training Loss: 0.2731, Validation Loss: 0.3824, Validation Accuracy: 86.97%\n",
            "Epoch [28/30], Training Loss: 0.2664, Validation Loss: 0.3817, Validation Accuracy: 87.21%\n",
            "Epoch [29/30], Training Loss: 0.2644, Validation Loss: 0.3860, Validation Accuracy: 87.33%\n",
            "Epoch [30/30], Training Loss: 0.2610, Validation Loss: 0.3754, Validation Accuracy: 87.39%\n",
            "\n",
            "Hyperparameter Tuning Results:\n",
            "{'activation': 'leaky_relu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 87.39}\n",
            "\n",
            "Best Model: {'activation': 'leaky_relu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 87.39}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut connection if input and output dimensions don't match\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)  # Residual connection\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "\n",
        "        # Global average pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)  # Global average pooling\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = ImprovedCNN(num_classes=100).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_B24yuJtmwM",
        "outputId": "3a8802dc-70bf-48ad-8c89-774b0686b7a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImprovedCNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50  # Train for more epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct / total\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Training Loss: {train_loss:.4f}, \"\n",
        "          f\"Validation Loss: {val_loss:.4f}, \"\n",
        "          f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step(val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkTY-jfbtmFX",
        "outputId": "5d0843ed-709c-4078-fd50-bbb17aa6afb6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Training Loss: 4.2032, Validation Loss: 3.9725, Validation Accuracy: 8.69%\n",
            "Epoch [2/50], Training Loss: 3.6940, Validation Loss: 3.3441, Validation Accuracy: 18.15%\n",
            "Epoch [3/50], Training Loss: 3.2683, Validation Loss: 3.0259, Validation Accuracy: 23.31%\n",
            "Epoch [4/50], Training Loss: 2.9182, Validation Loss: 2.6275, Validation Accuracy: 31.53%\n",
            "Epoch [5/50], Training Loss: 2.6165, Validation Loss: 2.4254, Validation Accuracy: 35.80%\n",
            "Epoch [6/50], Training Loss: 2.3894, Validation Loss: 2.2513, Validation Accuracy: 40.27%\n",
            "Epoch [7/50], Training Loss: 2.1925, Validation Loss: 2.0671, Validation Accuracy: 44.26%\n",
            "Epoch [8/50], Training Loss: 2.0294, Validation Loss: 1.9744, Validation Accuracy: 46.06%\n",
            "Epoch [9/50], Training Loss: 1.8836, Validation Loss: 1.8224, Validation Accuracy: 49.59%\n",
            "Epoch [10/50], Training Loss: 1.7624, Validation Loss: 1.7194, Validation Accuracy: 51.94%\n",
            "Epoch [11/50], Training Loss: 1.6451, Validation Loss: 1.6723, Validation Accuracy: 53.42%\n",
            "Epoch [12/50], Training Loss: 1.5414, Validation Loss: 1.6109, Validation Accuracy: 55.17%\n",
            "Epoch [13/50], Training Loss: 1.4557, Validation Loss: 1.5390, Validation Accuracy: 57.12%\n",
            "Epoch [14/50], Training Loss: 1.3663, Validation Loss: 1.5787, Validation Accuracy: 56.36%\n",
            "Epoch [15/50], Training Loss: 1.2838, Validation Loss: 1.5100, Validation Accuracy: 57.86%\n",
            "Epoch [16/50], Training Loss: 1.2109, Validation Loss: 1.4454, Validation Accuracy: 59.69%\n",
            "Epoch [17/50], Training Loss: 1.1489, Validation Loss: 1.4381, Validation Accuracy: 60.13%\n",
            "Epoch [18/50], Training Loss: 1.0812, Validation Loss: 1.4127, Validation Accuracy: 60.86%\n",
            "Epoch [19/50], Training Loss: 1.0257, Validation Loss: 1.3959, Validation Accuracy: 61.38%\n",
            "Epoch [20/50], Training Loss: 0.9717, Validation Loss: 1.4002, Validation Accuracy: 61.81%\n",
            "Epoch [21/50], Training Loss: 0.9145, Validation Loss: 1.4535, Validation Accuracy: 61.43%\n",
            "Epoch [22/50], Training Loss: 0.8659, Validation Loss: 1.3891, Validation Accuracy: 63.10%\n",
            "Epoch [23/50], Training Loss: 0.8023, Validation Loss: 1.4002, Validation Accuracy: 63.17%\n",
            "Epoch [24/50], Training Loss: 0.7756, Validation Loss: 1.4106, Validation Accuracy: 63.66%\n",
            "Epoch [25/50], Training Loss: 0.7308, Validation Loss: 1.4275, Validation Accuracy: 63.04%\n",
            "Epoch [26/50], Training Loss: 0.6798, Validation Loss: 1.4145, Validation Accuracy: 64.09%\n",
            "Epoch [27/50], Training Loss: 0.4939, Validation Loss: 1.2738, Validation Accuracy: 67.10%\n",
            "Epoch [28/50], Training Loss: 0.4245, Validation Loss: 1.2485, Validation Accuracy: 68.18%\n",
            "Epoch [29/50], Training Loss: 0.4011, Validation Loss: 1.2607, Validation Accuracy: 67.94%\n",
            "Epoch [30/50], Training Loss: 0.3788, Validation Loss: 1.2697, Validation Accuracy: 67.45%\n",
            "Epoch [31/50], Training Loss: 0.3625, Validation Loss: 1.2705, Validation Accuracy: 68.20%\n",
            "Epoch [32/50], Training Loss: 0.3472, Validation Loss: 1.2854, Validation Accuracy: 68.01%\n",
            "Epoch [33/50], Training Loss: 0.3287, Validation Loss: 1.2848, Validation Accuracy: 68.01%\n",
            "Epoch [34/50], Training Loss: 0.3201, Validation Loss: 1.2687, Validation Accuracy: 67.81%\n",
            "Epoch [35/50], Training Loss: 0.3241, Validation Loss: 1.2522, Validation Accuracy: 68.66%\n",
            "Epoch [36/50], Training Loss: 0.3151, Validation Loss: 1.2910, Validation Accuracy: 68.23%\n",
            "Epoch [37/50], Training Loss: 0.3095, Validation Loss: 1.2864, Validation Accuracy: 67.88%\n",
            "Epoch [38/50], Training Loss: 0.3126, Validation Loss: 1.2865, Validation Accuracy: 67.56%\n",
            "Epoch [39/50], Training Loss: 0.3071, Validation Loss: 1.2618, Validation Accuracy: 68.27%\n",
            "Epoch [40/50], Training Loss: 0.3116, Validation Loss: 1.2755, Validation Accuracy: 68.33%\n",
            "Epoch [41/50], Training Loss: 0.3096, Validation Loss: 1.2788, Validation Accuracy: 68.20%\n",
            "Epoch [42/50], Training Loss: 0.3048, Validation Loss: 1.2736, Validation Accuracy: 68.71%\n",
            "Epoch [43/50], Training Loss: 0.3122, Validation Loss: 1.2688, Validation Accuracy: 68.42%\n",
            "Epoch [44/50], Training Loss: 0.3051, Validation Loss: 1.2756, Validation Accuracy: 68.27%\n",
            "Epoch [45/50], Training Loss: 0.3102, Validation Loss: 1.2748, Validation Accuracy: 68.49%\n",
            "Epoch [46/50], Training Loss: 0.3110, Validation Loss: 1.2735, Validation Accuracy: 68.21%\n",
            "Epoch [47/50], Training Loss: 0.3126, Validation Loss: 1.2852, Validation Accuracy: 68.04%\n",
            "Epoch [48/50], Training Loss: 0.3107, Validation Loss: 1.2768, Validation Accuracy: 67.83%\n",
            "Epoch [49/50], Training Loss: 0.3105, Validation Loss: 1.2717, Validation Accuracy: 67.97%\n",
            "Epoch [50/50], Training Loss: 0.3087, Validation Loss: 1.2714, Validation Accuracy: 68.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-100 dataset"
      ],
      "metadata": {
        "id": "AcvyoX37Sifo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define transformations for the training and testing datasets\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Horizontal flipping\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jittering\n",
        "    transforms.RandomCrop(32, padding=4),  # Randomly crop the image\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),  # Normalize with CIFAR-100 mean and std\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),  # Normalize with CIFAR-100 mean and std\n",
        "])\n",
        "\n",
        "# Load the CIFAR-100 training dataset\n",
        "train_dataset = datasets.CIFAR100(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=True,  # Load the training set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transform_train,  # Apply the training transformations\n",
        ")\n",
        "\n",
        "# Load the CIFAR-100 testing dataset\n",
        "test_dataset = datasets.CIFAR100(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=False,  # Load the testing set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transform_test,  # Apply the testing transformations\n",
        ")\n",
        "\n",
        "# Split the training dataset into training and validation subsets\n",
        "train_size = int(0.8 * len(train_dataset))  # 80% for training\n",
        "val_size = len(train_dataset) - train_size  # 20% for validation\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, activation='relu', kernel_size=3, num_classes=100):\n",
        "        super(CNN, self).__init__()\n",
        "        self.activation = activation\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.fc1 = nn.Linear(256 * 8 * 8, 1024)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.apply_activation(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(self.apply_activation(self.bn2(self.conv2(x))))\n",
        "        x = self.apply_activation(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(self.apply_activation(self.bn4(self.conv4(x))))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(self.apply_activation(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def apply_activation(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return F.relu(x)\n",
        "        elif self.activation == 'leaky_relu':\n",
        "            return F.leaky_relu(x, negative_slope=0.1)\n",
        "        elif self.activation == 'gelu':\n",
        "            return F.gelu(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {self.activation}\")\n",
        "\n",
        "# Hyperparameters to tune\n",
        "activations = ['relu', 'leaky_relu', 'gelu']\n",
        "kernel_sizes = [3, 5]\n",
        "optimizers_list = ['adam', 'sgd']\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Hyperparameter tuning loop\n",
        "for activation in activations:\n",
        "    for kernel_size in kernel_sizes:\n",
        "        for optimizer_name in optimizers_list:\n",
        "            print(f\"\\nTraining with activation={activation}, kernel_size={kernel_size}, optimizer={optimizer_name}\")\n",
        "\n",
        "            # Create the model\n",
        "            model = CNN(activation=activation, kernel_size=kernel_size, num_classes=100).to(device)\n",
        "\n",
        "            # Define loss function and optimizer\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            if optimizer_name == 'adam':\n",
        "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "            elif optimizer_name == 'sgd':\n",
        "                optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "            # Training loop\n",
        "            num_epochs = 10  # Train for fewer epochs for tuning\n",
        "            best_val_accuracy = 0.0\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "                train_loss = running_loss / len(train_loader)\n",
        "                train_accuracy = 100 * correct / total\n",
        "\n",
        "                # Validation loop\n",
        "                model.eval()\n",
        "                val_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for inputs, labels in val_loader:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item()\n",
        "\n",
        "                        _, predicted = torch.max(outputs, 1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += (predicted == labels).sum().item()\n",
        "\n",
        "                val_loss = val_loss / len(val_loader)\n",
        "                val_accuracy = 100 * correct / total\n",
        "\n",
        "                # Save the best validation accuracy\n",
        "                if val_accuracy > best_val_accuracy:\n",
        "                    best_val_accuracy = val_accuracy\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "                      f\"Training Loss: {train_loss:.4f}, \"\n",
        "                      f\"Validation Loss: {val_loss:.4f}, \"\n",
        "                      f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "                # Step the learning rate scheduler\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "            # Store the results\n",
        "            results.append({\n",
        "                'activation': activation,\n",
        "                'kernel_size': kernel_size,\n",
        "                'optimizer': optimizer_name,\n",
        "                'best_val_accuracy': best_val_accuracy\n",
        "            })\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nHyperparameter Tuning Results:\")\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "# Find the best model\n",
        "best_result = max(results, key=lambda x: x['best_val_accuracy'])\n",
        "print(f\"\\nBest Model: {best_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grMaHmUbtyqY",
        "outputId": "0a98a607-70e4-48a8-9ddb-5ad6f91c1a50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Training with activation=relu, kernel_size=3, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 4.5777, Validation Loss: 4.2961, Validation Accuracy: 4.79%\n",
            "Epoch [2/10], Training Loss: 4.3150, Validation Loss: 4.1454, Validation Accuracy: 6.32%\n",
            "Epoch [3/10], Training Loss: 4.2476, Validation Loss: 4.0683, Validation Accuracy: 7.22%\n",
            "Epoch [4/10], Training Loss: 4.1798, Validation Loss: 3.9718, Validation Accuracy: 8.60%\n",
            "Epoch [5/10], Training Loss: 4.1246, Validation Loss: 3.8895, Validation Accuracy: 10.05%\n",
            "Epoch [6/10], Training Loss: 4.0665, Validation Loss: 3.8242, Validation Accuracy: 10.33%\n",
            "Epoch [7/10], Training Loss: 3.9957, Validation Loss: 3.7461, Validation Accuracy: 12.61%\n",
            "Epoch [8/10], Training Loss: 3.9337, Validation Loss: 3.6312, Validation Accuracy: 14.24%\n",
            "Epoch [9/10], Training Loss: 3.8619, Validation Loss: 3.5529, Validation Accuracy: 16.26%\n",
            "Epoch [10/10], Training Loss: 3.7936, Validation Loss: 3.4630, Validation Accuracy: 16.54%\n",
            "\n",
            "Training with activation=relu, kernel_size=3, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 4.1240, Validation Loss: 3.7581, Validation Accuracy: 12.02%\n",
            "Epoch [2/10], Training Loss: 3.8147, Validation Loss: 3.5128, Validation Accuracy: 15.42%\n",
            "Epoch [3/10], Training Loss: 3.6352, Validation Loss: 3.3182, Validation Accuracy: 19.36%\n",
            "Epoch [4/10], Training Loss: 3.4811, Validation Loss: 3.1584, Validation Accuracy: 21.92%\n",
            "Epoch [5/10], Training Loss: 3.3215, Validation Loss: 2.9901, Validation Accuracy: 24.64%\n",
            "Epoch [6/10], Training Loss: 3.1637, Validation Loss: 3.0385, Validation Accuracy: 24.37%\n",
            "Epoch [7/10], Training Loss: 3.0391, Validation Loss: 2.8713, Validation Accuracy: 27.77%\n",
            "Epoch [8/10], Training Loss: 2.9318, Validation Loss: 2.7023, Validation Accuracy: 30.64%\n",
            "Epoch [9/10], Training Loss: 2.8283, Validation Loss: 2.7513, Validation Accuracy: 29.24%\n",
            "Epoch [10/10], Training Loss: 2.7444, Validation Loss: 2.5674, Validation Accuracy: 34.14%\n",
            "\n",
            "Training with activation=relu, kernel_size=5, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 4.7307, Validation Loss: 4.5639, Validation Accuracy: 1.44%\n",
            "Epoch [2/10], Training Loss: 4.5582, Validation Loss: 4.5237, Validation Accuracy: 1.68%\n",
            "Epoch [3/10], Training Loss: 4.5403, Validation Loss: 4.4869, Validation Accuracy: 1.68%\n",
            "Epoch [4/10], Training Loss: 4.5310, Validation Loss: 4.4819, Validation Accuracy: 1.61%\n",
            "Epoch [5/10], Training Loss: 4.5251, Validation Loss: 4.4576, Validation Accuracy: 1.88%\n",
            "Epoch [6/10], Training Loss: 4.5188, Validation Loss: 4.4345, Validation Accuracy: 1.70%\n",
            "Epoch [7/10], Training Loss: 4.5170, Validation Loss: 4.4343, Validation Accuracy: 1.91%\n",
            "Epoch [8/10], Training Loss: 4.5119, Validation Loss: 4.4427, Validation Accuracy: 1.85%\n",
            "Epoch [9/10], Training Loss: 4.5098, Validation Loss: 4.4558, Validation Accuracy: 1.97%\n",
            "Epoch [10/10], Training Loss: 4.5093, Validation Loss: 4.4298, Validation Accuracy: 1.90%\n",
            "\n",
            "Training with activation=relu, kernel_size=5, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 4.1471, Validation Loss: 3.7886, Validation Accuracy: 10.76%\n",
            "Epoch [2/10], Training Loss: 3.8263, Validation Loss: 3.5210, Validation Accuracy: 15.85%\n",
            "Epoch [3/10], Training Loss: 3.6332, Validation Loss: 3.2847, Validation Accuracy: 19.45%\n",
            "Epoch [4/10], Training Loss: 3.4460, Validation Loss: 3.1583, Validation Accuracy: 22.18%\n",
            "Epoch [5/10], Training Loss: 3.2844, Validation Loss: 3.0723, Validation Accuracy: 23.16%\n",
            "Epoch [6/10], Training Loss: 3.1173, Validation Loss: 2.8343, Validation Accuracy: 28.29%\n",
            "Epoch [7/10], Training Loss: 2.9689, Validation Loss: 2.6954, Validation Accuracy: 31.07%\n",
            "Epoch [8/10], Training Loss: 2.8461, Validation Loss: 2.7024, Validation Accuracy: 30.78%\n",
            "Epoch [9/10], Training Loss: 2.7333, Validation Loss: 2.5178, Validation Accuracy: 35.18%\n",
            "Epoch [10/10], Training Loss: 2.6121, Validation Loss: 2.5283, Validation Accuracy: 34.15%\n",
            "\n",
            "Training with activation=leaky_relu, kernel_size=3, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 4.6228, Validation Loss: 3.9313, Validation Accuracy: 9.85%\n",
            "Epoch [2/10], Training Loss: 3.8152, Validation Loss: 3.4532, Validation Accuracy: 16.88%\n",
            "Epoch [3/10], Training Loss: 3.4293, Validation Loss: 3.2141, Validation Accuracy: 21.35%\n",
            "Epoch [4/10], Training Loss: 3.1335, Validation Loss: 2.9052, Validation Accuracy: 27.04%\n",
            "Epoch [5/10], Training Loss: 2.8913, Validation Loss: 2.6639, Validation Accuracy: 32.54%\n",
            "Epoch [6/10], Training Loss: 2.6877, Validation Loss: 2.5999, Validation Accuracy: 33.96%\n",
            "Epoch [7/10], Training Loss: 2.5156, Validation Loss: 2.4288, Validation Accuracy: 36.75%\n",
            "Epoch [8/10], Training Loss: 2.3715, Validation Loss: 2.2061, Validation Accuracy: 41.64%\n",
            "Epoch [9/10], Training Loss: 2.2322, Validation Loss: 2.1362, Validation Accuracy: 43.90%\n",
            "Epoch [10/10], Training Loss: 2.1220, Validation Loss: 2.0551, Validation Accuracy: 45.72%\n",
            "\n",
            "Training with activation=leaky_relu, kernel_size=3, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 4.0273, Validation Loss: 3.6128, Validation Accuracy: 14.12%\n",
            "Epoch [2/10], Training Loss: 3.5830, Validation Loss: 3.3670, Validation Accuracy: 18.20%\n",
            "Epoch [3/10], Training Loss: 3.2847, Validation Loss: 3.0060, Validation Accuracy: 24.98%\n",
            "Epoch [4/10], Training Loss: 3.0571, Validation Loss: 2.8461, Validation Accuracy: 28.28%\n",
            "Epoch [5/10], Training Loss: 2.8430, Validation Loss: 2.6821, Validation Accuracy: 31.64%\n",
            "Epoch [6/10], Training Loss: 2.6950, Validation Loss: 2.5360, Validation Accuracy: 35.13%\n",
            "Epoch [7/10], Training Loss: 2.5468, Validation Loss: 2.5673, Validation Accuracy: 34.85%\n",
            "Epoch [8/10], Training Loss: 2.4312, Validation Loss: 2.3457, Validation Accuracy: 38.79%\n",
            "Epoch [9/10], Training Loss: 2.3218, Validation Loss: 2.3272, Validation Accuracy: 40.07%\n",
            "Epoch [10/10], Training Loss: 2.2333, Validation Loss: 2.1887, Validation Accuracy: 42.26%\n",
            "\n",
            "Training with activation=leaky_relu, kernel_size=5, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 4.9544, Validation Loss: 4.2064, Validation Accuracy: 5.61%\n",
            "Epoch [2/10], Training Loss: 4.1179, Validation Loss: 3.8472, Validation Accuracy: 11.34%\n",
            "Epoch [3/10], Training Loss: 3.7540, Validation Loss: 3.5315, Validation Accuracy: 16.11%\n",
            "Epoch [4/10], Training Loss: 3.4714, Validation Loss: 3.2591, Validation Accuracy: 21.29%\n",
            "Epoch [5/10], Training Loss: 3.1843, Validation Loss: 3.0148, Validation Accuracy: 26.21%\n",
            "Epoch [6/10], Training Loss: 2.9363, Validation Loss: 2.6716, Validation Accuracy: 32.55%\n",
            "Epoch [7/10], Training Loss: 2.7243, Validation Loss: 2.5698, Validation Accuracy: 34.23%\n",
            "Epoch [8/10], Training Loss: 2.5657, Validation Loss: 2.4873, Validation Accuracy: 35.85%\n",
            "Epoch [9/10], Training Loss: 2.4228, Validation Loss: 2.2811, Validation Accuracy: 40.38%\n",
            "Epoch [10/10], Training Loss: 2.3035, Validation Loss: 2.2632, Validation Accuracy: 40.85%\n",
            "\n",
            "Training with activation=leaky_relu, kernel_size=5, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 4.0556, Validation Loss: 3.7173, Validation Accuracy: 12.56%\n",
            "Epoch [2/10], Training Loss: 3.6058, Validation Loss: 3.3768, Validation Accuracy: 18.57%\n",
            "Epoch [3/10], Training Loss: 3.3011, Validation Loss: 3.0235, Validation Accuracy: 25.20%\n",
            "Epoch [4/10], Training Loss: 3.0476, Validation Loss: 2.9184, Validation Accuracy: 27.85%\n",
            "Epoch [5/10], Training Loss: 2.8249, Validation Loss: 2.6529, Validation Accuracy: 32.74%\n",
            "Epoch [6/10], Training Loss: 2.6469, Validation Loss: 2.4870, Validation Accuracy: 35.69%\n",
            "Epoch [7/10], Training Loss: 2.5022, Validation Loss: 2.4642, Validation Accuracy: 36.87%\n",
            "Epoch [8/10], Training Loss: 2.3748, Validation Loss: 2.2797, Validation Accuracy: 40.72%\n",
            "Epoch [9/10], Training Loss: 2.2781, Validation Loss: 2.1918, Validation Accuracy: 42.13%\n",
            "Epoch [10/10], Training Loss: 2.1712, Validation Loss: 2.1596, Validation Accuracy: 42.93%\n",
            "\n",
            "Training with activation=gelu, kernel_size=3, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 4.4065, Validation Loss: 4.0196, Validation Accuracy: 7.83%\n",
            "Epoch [2/10], Training Loss: 4.0529, Validation Loss: 3.8036, Validation Accuracy: 11.41%\n",
            "Epoch [3/10], Training Loss: 3.8894, Validation Loss: 3.5877, Validation Accuracy: 14.02%\n",
            "Epoch [4/10], Training Loss: 3.7204, Validation Loss: 3.3839, Validation Accuracy: 18.09%\n",
            "Epoch [5/10], Training Loss: 3.5804, Validation Loss: 3.2820, Validation Accuracy: 19.73%\n",
            "Epoch [6/10], Training Loss: 3.4395, Validation Loss: 3.0884, Validation Accuracy: 23.85%\n",
            "Epoch [7/10], Training Loss: 3.2924, Validation Loss: 2.9301, Validation Accuracy: 27.26%\n",
            "Epoch [8/10], Training Loss: 3.1587, Validation Loss: 2.7981, Validation Accuracy: 29.68%\n",
            "Epoch [9/10], Training Loss: 3.0500, Validation Loss: 2.7088, Validation Accuracy: 31.60%\n",
            "Epoch [10/10], Training Loss: 2.9364, Validation Loss: 2.5926, Validation Accuracy: 33.49%\n",
            "\n",
            "Training with activation=gelu, kernel_size=3, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 4.0573, Validation Loss: 3.6432, Validation Accuracy: 13.84%\n",
            "Epoch [2/10], Training Loss: 3.6167, Validation Loss: 3.3571, Validation Accuracy: 19.50%\n",
            "Epoch [3/10], Training Loss: 3.3312, Validation Loss: 3.0142, Validation Accuracy: 24.77%\n",
            "Epoch [4/10], Training Loss: 3.0773, Validation Loss: 2.7960, Validation Accuracy: 29.39%\n",
            "Epoch [5/10], Training Loss: 2.8947, Validation Loss: 2.7152, Validation Accuracy: 31.48%\n",
            "Epoch [6/10], Training Loss: 2.7294, Validation Loss: 2.4832, Validation Accuracy: 35.47%\n",
            "Epoch [7/10], Training Loss: 2.5919, Validation Loss: 2.4927, Validation Accuracy: 36.14%\n",
            "Epoch [8/10], Training Loss: 2.4714, Validation Loss: 2.3550, Validation Accuracy: 38.51%\n",
            "Epoch [9/10], Training Loss: 2.3781, Validation Loss: 2.2073, Validation Accuracy: 41.97%\n",
            "Epoch [10/10], Training Loss: 2.2886, Validation Loss: 2.2925, Validation Accuracy: 40.28%\n",
            "\n",
            "Training with activation=gelu, kernel_size=5, optimizer=adam\n",
            "Epoch [1/10], Training Loss: 4.4586, Validation Loss: 4.1046, Validation Accuracy: 6.56%\n",
            "Epoch [2/10], Training Loss: 4.1609, Validation Loss: 3.9388, Validation Accuracy: 9.05%\n",
            "Epoch [3/10], Training Loss: 4.0293, Validation Loss: 3.8148, Validation Accuracy: 11.20%\n",
            "Epoch [4/10], Training Loss: 3.9166, Validation Loss: 3.6838, Validation Accuracy: 14.38%\n",
            "Epoch [5/10], Training Loss: 3.7881, Validation Loss: 3.4885, Validation Accuracy: 17.27%\n",
            "Epoch [6/10], Training Loss: 3.6420, Validation Loss: 3.3588, Validation Accuracy: 19.14%\n",
            "Epoch [7/10], Training Loss: 3.5102, Validation Loss: 3.2195, Validation Accuracy: 21.04%\n",
            "Epoch [8/10], Training Loss: 3.3628, Validation Loss: 2.9986, Validation Accuracy: 25.57%\n",
            "Epoch [9/10], Training Loss: 3.2414, Validation Loss: 2.8835, Validation Accuracy: 27.54%\n",
            "Epoch [10/10], Training Loss: 3.0978, Validation Loss: 2.7788, Validation Accuracy: 29.22%\n",
            "\n",
            "Training with activation=gelu, kernel_size=5, optimizer=sgd\n",
            "Epoch [1/10], Training Loss: 4.0156, Validation Loss: 3.6023, Validation Accuracy: 14.19%\n",
            "Epoch [2/10], Training Loss: 3.5715, Validation Loss: 3.2591, Validation Accuracy: 20.48%\n",
            "Epoch [3/10], Training Loss: 3.2536, Validation Loss: 2.9045, Validation Accuracy: 26.97%\n",
            "Epoch [4/10], Training Loss: 2.9777, Validation Loss: 2.7865, Validation Accuracy: 30.21%\n",
            "Epoch [5/10], Training Loss: 2.7563, Validation Loss: 2.7232, Validation Accuracy: 30.79%\n",
            "Epoch [6/10], Training Loss: 2.5812, Validation Loss: 2.5681, Validation Accuracy: 34.78%\n",
            "Epoch [7/10], Training Loss: 2.4250, Validation Loss: 2.4850, Validation Accuracy: 35.60%\n",
            "Epoch [8/10], Training Loss: 2.2930, Validation Loss: 2.3359, Validation Accuracy: 39.80%\n",
            "Epoch [9/10], Training Loss: 2.1829, Validation Loss: 2.1220, Validation Accuracy: 43.84%\n",
            "Epoch [10/10], Training Loss: 2.0803, Validation Loss: 2.1352, Validation Accuracy: 43.73%\n",
            "\n",
            "Hyperparameter Tuning Results:\n",
            "{'activation': 'relu', 'kernel_size': 3, 'optimizer': 'adam', 'best_val_accuracy': 16.54}\n",
            "{'activation': 'relu', 'kernel_size': 3, 'optimizer': 'sgd', 'best_val_accuracy': 34.14}\n",
            "{'activation': 'relu', 'kernel_size': 5, 'optimizer': 'adam', 'best_val_accuracy': 1.97}\n",
            "{'activation': 'relu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 35.18}\n",
            "{'activation': 'leaky_relu', 'kernel_size': 3, 'optimizer': 'adam', 'best_val_accuracy': 45.72}\n",
            "{'activation': 'leaky_relu', 'kernel_size': 3, 'optimizer': 'sgd', 'best_val_accuracy': 42.26}\n",
            "{'activation': 'leaky_relu', 'kernel_size': 5, 'optimizer': 'adam', 'best_val_accuracy': 40.85}\n",
            "{'activation': 'leaky_relu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 42.93}\n",
            "{'activation': 'gelu', 'kernel_size': 3, 'optimizer': 'adam', 'best_val_accuracy': 33.49}\n",
            "{'activation': 'gelu', 'kernel_size': 3, 'optimizer': 'sgd', 'best_val_accuracy': 41.97}\n",
            "{'activation': 'gelu', 'kernel_size': 5, 'optimizer': 'adam', 'best_val_accuracy': 29.22}\n",
            "{'activation': 'gelu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 43.84}\n",
            "\n",
            "Best Model: {'activation': 'leaky_relu', 'kernel_size': 3, 'optimizer': 'adam', 'best_val_accuracy': 45.72}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define transformations for the training and testing datasets\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Horizontal flipping\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color jittering\n",
        "    transforms.RandomCrop(32, padding=4),  # Randomly crop the image\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),  # Normalize with CIFAR-100 mean and std\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),  # Normalize with CIFAR-100 mean and std\n",
        "])\n",
        "\n",
        "# Load the CIFAR-100 training dataset\n",
        "train_dataset = datasets.CIFAR100(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=True,  # Load the training set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transform_train,  # Apply the training transformations\n",
        ")\n",
        "\n",
        "# Load the CIFAR-100 testing dataset\n",
        "test_dataset = datasets.CIFAR100(\n",
        "    root='./data',  # Path to store the dataset\n",
        "    train=False,  # Load the testing set\n",
        "    download=True,  # Download the dataset if it doesn't exist\n",
        "    transform=transform_test,  # Apply the testing transformations\n",
        ")\n",
        "\n",
        "# Split the training dataset into training and validation subsets\n",
        "train_size = int(0.8 * len(train_dataset))  # 80% for training\n",
        "val_size = len(train_dataset) - train_size  # 20% for validation\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, activation='relu', kernel_size=3, num_classes=100):\n",
        "        super(CNN, self).__init__()\n",
        "        self.activation = activation\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.fc1 = nn.Linear(256 * 8 * 8, 1024)\n",
        "        self.fc2 = nn.Linear(1024, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.apply_activation(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(self.apply_activation(self.bn2(self.conv2(x))))\n",
        "        x = self.apply_activation(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(self.apply_activation(self.bn4(self.conv4(x))))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout(self.apply_activation(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def apply_activation(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return F.relu(x)\n",
        "        elif self.activation == 'leaky_relu':\n",
        "            return F.leaky_relu(x, negative_slope=0.1)\n",
        "        elif self.activation == 'gelu':\n",
        "            return F.gelu(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {self.activation}\")\n",
        "\n",
        "# Hyperparameters to tune\n",
        "activations = [ 'gelu']\n",
        "kernel_sizes = [ 5]\n",
        "optimizers_list = [ 'sgd']\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Hyperparameter tuning loop\n",
        "for activation in activations:\n",
        "    for kernel_size in kernel_sizes:\n",
        "        for optimizer_name in optimizers_list:\n",
        "            print(f\"\\nTraining with activation={activation}, kernel_size={kernel_size}, optimizer={optimizer_name}\")\n",
        "\n",
        "            # Create the model\n",
        "            model = CNN(activation=activation, kernel_size=kernel_size, num_classes=100).to(device)\n",
        "\n",
        "            # Define loss function and optimizer\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            if optimizer_name == 'adam':\n",
        "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "            elif optimizer_name == 'sgd':\n",
        "                optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "            # Training loop\n",
        "            num_epochs = 50  # Train for fewer epochs for tuning\n",
        "            best_val_accuracy = 0.0\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "                train_loss = running_loss / len(train_loader)\n",
        "                train_accuracy = 100 * correct / total\n",
        "\n",
        "                # Validation loop\n",
        "                model.eval()\n",
        "                val_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for inputs, labels in val_loader:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        val_loss += loss.item()\n",
        "\n",
        "                        _, predicted = torch.max(outputs, 1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += (predicted == labels).sum().item()\n",
        "\n",
        "                val_loss = val_loss / len(val_loader)\n",
        "                val_accuracy = 100 * correct / total\n",
        "\n",
        "                # Save the best validation accuracy\n",
        "                if val_accuracy > best_val_accuracy:\n",
        "                    best_val_accuracy = val_accuracy\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "                      f\"Training Loss: {train_loss:.4f}, \"\n",
        "                      f\"Validation Loss: {val_loss:.4f}, \"\n",
        "                      f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "                # Step the learning rate scheduler\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "            # Store the results\n",
        "            results.append({\n",
        "                'activation': activation,\n",
        "                'kernel_size': kernel_size,\n",
        "                'optimizer': optimizer_name,\n",
        "                'best_val_accuracy': best_val_accuracy\n",
        "            })\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nHyperparameter Tuning Results:\")\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "# Find the best model\n",
        "best_result = max(results, key=lambda x: x['best_val_accuracy'])\n",
        "print(f\"\\nBest Model: {best_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPe286CzgpDe",
        "outputId": "db8fa0bf-4272-497e-9bde-af55ec3ee9ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Training with activation=gelu, kernel_size=5, optimizer=sgd\n",
            "Epoch [1/50], Training Loss: 4.0089, Validation Loss: 3.6558, Validation Accuracy: 13.93%\n",
            "Epoch [2/50], Training Loss: 3.5497, Validation Loss: 3.2464, Validation Accuracy: 20.96%\n",
            "Epoch [3/50], Training Loss: 3.2309, Validation Loss: 3.0356, Validation Accuracy: 25.17%\n",
            "Epoch [4/50], Training Loss: 2.9520, Validation Loss: 2.9111, Validation Accuracy: 27.18%\n",
            "Epoch [5/50], Training Loss: 2.7335, Validation Loss: 2.5851, Validation Accuracy: 33.61%\n",
            "Epoch [6/50], Training Loss: 2.5677, Validation Loss: 2.4664, Validation Accuracy: 36.38%\n",
            "Epoch [7/50], Training Loss: 2.4014, Validation Loss: 2.5264, Validation Accuracy: 35.32%\n",
            "Epoch [8/50], Training Loss: 2.2835, Validation Loss: 2.2287, Validation Accuracy: 41.55%\n",
            "Epoch [9/50], Training Loss: 2.1771, Validation Loss: 2.1104, Validation Accuracy: 43.58%\n",
            "Epoch [10/50], Training Loss: 2.0817, Validation Loss: 2.0818, Validation Accuracy: 44.45%\n",
            "Epoch [11/50], Training Loss: 1.9803, Validation Loss: 1.9668, Validation Accuracy: 47.68%\n",
            "Epoch [12/50], Training Loss: 1.9037, Validation Loss: 1.9610, Validation Accuracy: 47.53%\n",
            "Epoch [13/50], Training Loss: 1.8282, Validation Loss: 1.9458, Validation Accuracy: 48.07%\n",
            "Epoch [14/50], Training Loss: 1.7567, Validation Loss: 1.8305, Validation Accuracy: 51.00%\n",
            "Epoch [15/50], Training Loss: 1.6929, Validation Loss: 1.8945, Validation Accuracy: 49.68%\n",
            "Epoch [16/50], Training Loss: 1.6325, Validation Loss: 1.8298, Validation Accuracy: 50.77%\n",
            "Epoch [17/50], Training Loss: 1.5773, Validation Loss: 1.7361, Validation Accuracy: 52.79%\n",
            "Epoch [18/50], Training Loss: 1.5227, Validation Loss: 1.7244, Validation Accuracy: 53.14%\n",
            "Epoch [19/50], Training Loss: 1.4679, Validation Loss: 1.6817, Validation Accuracy: 53.52%\n",
            "Epoch [20/50], Training Loss: 1.4186, Validation Loss: 1.6970, Validation Accuracy: 54.40%\n",
            "Epoch [21/50], Training Loss: 1.3712, Validation Loss: 1.6654, Validation Accuracy: 54.58%\n",
            "Epoch [22/50], Training Loss: 1.3308, Validation Loss: 1.6425, Validation Accuracy: 55.33%\n",
            "Epoch [23/50], Training Loss: 1.2953, Validation Loss: 1.6116, Validation Accuracy: 56.31%\n",
            "Epoch [24/50], Training Loss: 1.2570, Validation Loss: 1.6620, Validation Accuracy: 55.65%\n",
            "Epoch [25/50], Training Loss: 1.2149, Validation Loss: 1.5759, Validation Accuracy: 57.15%\n",
            "Epoch [26/50], Training Loss: 1.1637, Validation Loss: 1.5942, Validation Accuracy: 57.20%\n",
            "Epoch [27/50], Training Loss: 1.1313, Validation Loss: 1.5582, Validation Accuracy: 58.19%\n",
            "Epoch [28/50], Training Loss: 1.1066, Validation Loss: 1.5478, Validation Accuracy: 58.12%\n",
            "Epoch [29/50], Training Loss: 1.0635, Validation Loss: 1.6158, Validation Accuracy: 57.23%\n",
            "Epoch [30/50], Training Loss: 1.0338, Validation Loss: 1.5450, Validation Accuracy: 58.55%\n",
            "Epoch [31/50], Training Loss: 1.0034, Validation Loss: 1.5072, Validation Accuracy: 59.19%\n",
            "Epoch [32/50], Training Loss: 0.9826, Validation Loss: 1.5354, Validation Accuracy: 58.07%\n",
            "Epoch [33/50], Training Loss: 0.9460, Validation Loss: 1.5360, Validation Accuracy: 58.57%\n",
            "Epoch [34/50], Training Loss: 0.9122, Validation Loss: 1.5150, Validation Accuracy: 58.98%\n",
            "Epoch [35/50], Training Loss: 0.8824, Validation Loss: 1.5008, Validation Accuracy: 59.26%\n",
            "Epoch [36/50], Training Loss: 0.8625, Validation Loss: 1.5205, Validation Accuracy: 58.91%\n",
            "Epoch [37/50], Training Loss: 0.8369, Validation Loss: 1.4888, Validation Accuracy: 60.29%\n",
            "Epoch [38/50], Training Loss: 0.8070, Validation Loss: 1.5056, Validation Accuracy: 59.88%\n",
            "Epoch [39/50], Training Loss: 0.7935, Validation Loss: 1.4997, Validation Accuracy: 60.14%\n",
            "Epoch [40/50], Training Loss: 0.7701, Validation Loss: 1.5186, Validation Accuracy: 59.89%\n",
            "Epoch [41/50], Training Loss: 0.7513, Validation Loss: 1.4961, Validation Accuracy: 60.45%\n",
            "Epoch [42/50], Training Loss: 0.6395, Validation Loss: 1.3982, Validation Accuracy: 62.53%\n",
            "Epoch [43/50], Training Loss: 0.6035, Validation Loss: 1.3914, Validation Accuracy: 63.01%\n",
            "Epoch [44/50], Training Loss: 0.5874, Validation Loss: 1.3973, Validation Accuracy: 63.08%\n",
            "Epoch [45/50], Training Loss: 0.5794, Validation Loss: 1.3879, Validation Accuracy: 63.55%\n",
            "Epoch [46/50], Training Loss: 0.5715, Validation Loss: 1.3821, Validation Accuracy: 62.96%\n",
            "Epoch [47/50], Training Loss: 0.5574, Validation Loss: 1.3892, Validation Accuracy: 63.51%\n",
            "Epoch [48/50], Training Loss: 0.5531, Validation Loss: 1.3818, Validation Accuracy: 63.69%\n",
            "Epoch [49/50], Training Loss: 0.5415, Validation Loss: 1.3831, Validation Accuracy: 63.43%\n",
            "Epoch [50/50], Training Loss: 0.5333, Validation Loss: 1.3946, Validation Accuracy: 63.35%\n",
            "\n",
            "Hyperparameter Tuning Results:\n",
            "{'activation': 'gelu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 63.69}\n",
            "\n",
            "Best Model: {'activation': 'gelu', 'kernel_size': 5, 'optimizer': 'sgd', 'best_val_accuracy': 63.69}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AzSLH-ZIgnrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bktjEbmFgmGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "URydZfL7txuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wc2hjPQ0wZoC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}